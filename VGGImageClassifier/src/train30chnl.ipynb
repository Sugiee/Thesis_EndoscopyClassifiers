{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processDataset():\n",
    "    def __init__(self, datasetPath, resize_size):\n",
    "        self.uniqueClassNames = []   # [class1, class 2, class3, ...]\n",
    "        self.imagesAndLabels = []    # [{images: tensor(30x224x224), label: class1}, ...]\n",
    "        self.classNameToIndex = {}   # {class1: 0, class2: 1, class3: 2, ...}\n",
    "        self.IndexToClassName = {}   # {0: class1, 1: class2, 2: class3, ...}\n",
    "        self.allImages = []          # [tensor(30x224x224), tensor(30x224x224), tensor(30x224x224)]\n",
    "        self.allLabels = []          # [class1, class3, class3, ...]\n",
    "        self.transform = None\n",
    "        self.NumFramesPerVid = 10\n",
    "        \n",
    "        # get class\n",
    "        for className in os.listdir(datasetPath):\n",
    "            if os.path.isdir(os.path.join(datasetPath, className)):\n",
    "                self.uniqueClassNames.append(className)\n",
    "        \n",
    "        self.uniqueClassNames = set(self.uniqueClassNames)\n",
    "        \n",
    "        # (className -> index) & (index -> className) dict (unique)\n",
    "        self.classNameToIndex = {className: i for i, className in enumerate(self.uniqueClassNames)}\n",
    "        self.IndexToClassName = {i: className for i, className in enumerate(self.uniqueClassNames)}            \n",
    "        \n",
    "        # Image augmentation and normalization transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((resize_size, resize_size)),  # Resizing for VGG input\n",
    "            transforms.ToTensor(),          # Convert to tensor\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization\n",
    "        ])\n",
    "        \n",
    "        # Collect Image Groups\n",
    "        for className in self.uniqueClassNames:\n",
    "            currClassFolder = os.path.join(datasetPath, className)\n",
    "            allImagesInFolder = sorted([\n",
    "                    f for f in os.listdir(currClassFolder)\n",
    "                    if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "                ])\n",
    "        \n",
    "            # Group images by their file prefix\n",
    "            imageGroups = {}\n",
    "            for currImageName in allImagesInFolder:\n",
    "                prefix = '_'.join(currImageName.split('_')[:-1])\n",
    "                if prefix not in imageGroups:\n",
    "                    imageGroups[prefix] = []\n",
    "                imageGroups[prefix].append(currImageName)\n",
    "                \n",
    "            # For each group (video), we only proceed if exactly 10 frames exist\n",
    "            for prefix, sortedImageList in imageGroups.items():\n",
    "                if len(sortedImageList) == self.NumFramesPerVid:\n",
    "                    # Sort frames by index (0001, 0002, ...)\n",
    "                    sortedImageList = sorted(sortedImageList)\n",
    "                    stackedImages = []\n",
    "            \n",
    "                    # Apply transform to each frame and stack them\n",
    "                    for imName in sortedImageList:\n",
    "                        imPath = os.path.join(currClassFolder, imName)\n",
    "                        img = Image.open(imPath).convert('RGB')\n",
    "                        img = self.transform(img)   # shape: [3, 224, 224]\n",
    "                        stackedImages.append(img)\n",
    "\n",
    "                    # concat along channel dimension => [30, 224, 224]\n",
    "                    # (10 frames Ã— 3 channels each = 30 channels)\n",
    "                    stackedTensor = torch.cat(stackedImages, dim=0)\n",
    "\n",
    "                    self.imagesAndLabels.append({\n",
    "                        \"images\": stackedTensor,\n",
    "                        \"label\": className\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping {prefix} because it does not have exactly {self.NumFramesPerVid} images (found {len(sortedImageList)}).\")                \n",
    "                \n",
    "        for item in self.imagesAndLabels:\n",
    "                self.allImages.append(item[\"images\"])\n",
    "                self.allLabels.append(self.classNameToIndex[item[\"label\"]])   \n",
    "        \n",
    "        train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "            self.allImages, \n",
    "            self.allLabels, \n",
    "            test_size=0.2, \n",
    "            random_state=42            \n",
    "        )\n",
    "\n",
    "        self.train_images = torch.stack(train_images, dim=0)  # [N, 30, 224, 224]\n",
    "        self.val_images   = torch.stack(val_images,   dim=0)  # [N, 30, 224, 224]\n",
    "        self.train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "        self.val_labels   = torch.tensor(val_labels,   dtype=torch.long) \n",
    "        \n",
    "        # Compute Class Weights\n",
    "        train_labels_np = self.train_labels.numpy()\n",
    "        classes = np.unique(train_labels_np)  \n",
    "        class_weights = compute_class_weight(\n",
    "            class_weight='balanced', \n",
    "            classes=classes, \n",
    "            y=train_labels_np\n",
    "        )        \n",
    "        \n",
    "        _, label_count = np.unique(self.train_labels, return_counts=True)\n",
    "\n",
    "        # Convert to a PyTorch tensor and move to device\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "        \n",
    "        print(\"Class Weights (on device):\", self.class_weights)\n",
    "        print(\"Class:\", label_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualGeometryGroup(nn.Module):\n",
    "    def __init__(self, output_dim, resize_size):\n",
    "        super(VisualGeometryGroup, self).__init__()        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(30, 64, kernel_size=3, padding=1),   # Conv64 (takes 10images * 3RGB = 30 feature chnls)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # MaxPool\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Conv128\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # MaxPool\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), # Conv256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1), # Conv256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # MaxPool\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1), # Conv512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # Conv512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # MaxPool\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # Conv512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), # Conv512\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),         # MaxPool\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(            \n",
    "            nn.Linear(512 * int(resize_size/32) * int(resize_size/32), 4096),  # Adjust based on input size\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, output_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_sizes = [8, 16, 32]\n",
    "resize_sizes = [128, 224, 256, 320] \n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001] \n",
    "epochs = 50\n",
    "patience = 10\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(batch_sizes, resize_sizes, learning_rates))\n",
    "\n",
    "# Base directory where output folders will be created\n",
    "datasetPath = r\"\"\n",
    "base_output_dir = r\"\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "\n",
    "# Global trackers for best result\n",
    "maxValAcc = 0.0\n",
    "maxValAccFolder = None\n",
    "\n",
    "prevResize_Size = 0\n",
    "\n",
    "# Iterate over hyperparameter combinations\n",
    "for index, (batch_size, resize_size, lr) in enumerate(hyperparameter_combinations):\n",
    "    folder_name = f\"{index + 1:03d}\"  # e.g., '001', '002', etc.\n",
    "    newFolderPath = os.path.join(base_output_dir, folder_name)\n",
    "    os.makedirs(newFolderPath, exist_ok=True)\n",
    "        \n",
    "    # Logging\n",
    "    epochLogs = []\n",
    "    reportText = []    \n",
    "    \n",
    "    print(f\"Running combination {index + 1}: Batch Size={batch_size}, Resize={resize_size}, LR={lr}\")\n",
    "\n",
    "    # Initialise/re-initialise dataset only when resize_size changes\n",
    "    if prevResize_Size != resize_size:\n",
    "        dataset = processDataset(datasetPath, resize_size) # saves time by executing only when needed\n",
    "    prevResize_Size = resize_size\n",
    "    \n",
    "    # Initialise model and lossFn, optimiser\n",
    "    model = VisualGeometryGroup(output_dim=len(dataset.uniqueClassNames), resize_size=resize_size).to(device)\n",
    "    lossFn = nn.CrossEntropyLoss(weight=dataset.class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Variables for early stopping\n",
    "    bestValAccuracyForConfig = 0.0\n",
    "    bestModelStateForConfig = copy.deepcopy(model.state_dict())\n",
    "    epochsNoImprove = 0\n",
    "    \n",
    "    # Train and validate model\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_loss_total = 0.0\n",
    "\n",
    "        for i in range(0, len(dataset.train_images), batch_size):\n",
    "            Xbatch = dataset.train_images[i : i + batch_size].to(device)\n",
    "            ybatch = dataset.train_labels[i : i + batch_size].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()           # Clear old gradients\n",
    "            y_pred = model(Xbatch)          # Forward pass          \n",
    "            loss = lossFn(y_pred, ybatch)   # Compute loss                                    \n",
    "            loss.backward()                 # Backpropagation\n",
    "            optimizer.step()                # Update parameters\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            predictions = torch.argmax(y_pred, dim=1)\n",
    "            train_correct += (predictions == ybatch).sum().item()\n",
    "            train_total += ybatch.size(0)\n",
    "            train_loss_total += loss.item()\n",
    "        \n",
    "        # Training accuracy for the epoch\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        number_of_batches = len(dataset.train_images) / batch_size\n",
    "        train_loss_avg = train_loss_total / number_of_batches\n",
    "        \n",
    "        # Validate & save outputs\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        numValSamples = 0\n",
    "        val_loss_total = 0 # loss over multiple batches\n",
    "        \n",
    "        with torch.no_grad():                \n",
    "            for j in range(0, len(dataset.val_images), batch_size):\n",
    "                Xval = dataset.val_images[j : j + batch_size].to(device)\n",
    "                yval = dataset.val_labels[j : j + batch_size].to(device)            \n",
    "                \n",
    "                y_pred_val = model(Xval)\n",
    "\n",
    "                # Calculate validation loss & accuracy\n",
    "                val_loss_total += lossFn(y_pred_val, yval).item()\n",
    "                val_predictions = torch.argmax(y_pred_val, dim=1)            \n",
    "                val_correct += (val_predictions == yval).sum().item()\n",
    "                numValSamples += yval.size(0)\n",
    "                                \n",
    "        val_accuracy = 100 * val_correct / numValSamples\n",
    "        number_of_batches = len(dataset.val_images) / batch_size\n",
    "        val_loss_avg = val_loss_total / number_of_batches\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Acc: {train_accuracy:.4f}%, Train loss: {train_loss_avg:.4f}, \"\n",
    "                                          f\"Val Acc: {val_accuracy:.4f}%, Val loss: {val_loss_avg:.4f}\")                \n",
    "        epoch_line = (f\"Epoch {epoch+1}/{epochs}, \"\n",
    "                        f\"Train Acc: {train_accuracy:.4f}%, \" f\"Train loss: {train_loss_avg:.4f}, \"\n",
    "                        f\"Val Acc: {val_accuracy:.4f}%, Val loss: {val_loss_avg:.4f}\")   \n",
    "        epochLogs.append(epoch_line)\n",
    "\n",
    "        # Check if this is the best validation accuracy so far for curr config\n",
    "        if val_accuracy > bestValAccuracyForConfig:\n",
    "            bestValAccuracyForConfig = val_accuracy\n",
    "            bestModelStateForConfig = copy.deepcopy(model.state_dict())\n",
    "            epochsNoImprove = 0\n",
    "        else:\n",
    "            epochsNoImprove += 1\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochsNoImprove >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (no improvement in {patience} consecutive epochs).\")\n",
    "            break\n",
    "    \n",
    "    # Inference    \n",
    "    model.load_state_dict(bestModelStateForConfig)\n",
    "    \n",
    "    # Evaluate on the validation set using the best model\n",
    "    model.eval()    \n",
    "    valPredsList = [] # predictions over multiple batches\n",
    "    valLabelsList = []\n",
    "    \n",
    "    with torch.no_grad():                \n",
    "            for j in range(0, len(dataset.val_images), batch_size):\n",
    "                Xval = dataset.val_images[j : j + batch_size].to(device)\n",
    "                yval = dataset.val_labels[j : j + batch_size].to(device)            \n",
    "                \n",
    "                y_pred_val = model(Xval)\n",
    "                val_predictions = torch.argmax(y_pred_val, dim=1)         \n",
    "                \n",
    "                valPredsList.extend(val_predictions.cpu().numpy())\n",
    "                valLabelsList.extend(yval.cpu().numpy())\n",
    "    \n",
    "    # Confusion matrix & classification report\n",
    "    valPredsArray  = np.array(valPredsList)\n",
    "    valLabelsArray = np.array(valLabelsList)\n",
    "    \n",
    "    # Identify which classes actually appear in validation\n",
    "    valClassesInUse = np.unique(valLabelsArray)\n",
    "\n",
    "    # Map numeric labels (0,1,2,...) to string names        \n",
    "    matchedValLabelsNameArray = [dataset.IndexToClassName[i] for i in valClassesInUse]\n",
    "\n",
    "    sorted_indices = np.argsort(matchedValLabelsNameArray)\n",
    "    valClassNamesSorted = [matchedValLabelsNameArray[idx] for idx in sorted_indices]\n",
    "    valClassesInUse_sorted = valClassesInUse[sorted_indices]\n",
    "\n",
    "    valConfMatrix = confusion_matrix(\n",
    "        valLabelsArray,\n",
    "        valPredsArray,\n",
    "        labels=valClassesInUse_sorted\n",
    "    )\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 8))          # Increase figure size for large # of classes\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=valConfMatrix,\n",
    "        display_labels=valClassNamesSorted\n",
    "    )\n",
    "    disp.plot(include_values=True, \n",
    "                cmap=plt.cm.Blues,\n",
    "                xticks_rotation='vertical'\n",
    "    )            \n",
    "    confusionMatrixPath = os.path.join(newFolderPath, \"confusion_matrix.png\")\n",
    "    plt.savefig(confusionMatrixPath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Classification report\n",
    "    stringLabelsForReport = [dataset.IndexToClassName[i] for i in valClassesInUse]\n",
    "    class_report = classification_report(\n",
    "        valLabelsArray,\n",
    "        valPredsArray,\n",
    "        labels=valClassesInUse,        \n",
    "        targetNames=stringLabelsForReport,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Compute Per-Class TP, FP, FN, TN\n",
    "    #num_classes = len(valClassesInUse)\n",
    "    totalSamples = valConfMatrix.sum()\n",
    "    tp_fp_fn_tn_lines = [\"Per-Class TP/FP/FN/TN:\"]\n",
    "\n",
    "    for i, class_label in enumerate(valClassesInUse):\n",
    "        TP = valConfMatrix[i, i]\n",
    "        FP = valConfMatrix[:, i].sum() - TP\n",
    "        FN = valConfMatrix[i, :].sum() - TP\n",
    "        TN = totalSamples - (TP + FP + FN)\n",
    "        \n",
    "        line = (f\"Class {class_label} --> \"\n",
    "                f\"TP: {TP}, FP: {FP}, FN: {FN}, TN: {TN}\")\n",
    "        tp_fp_fn_tn_lines.append(line)\n",
    "\n",
    "    tp_fp_fn_tn_report = \"\\n\".join(tp_fp_fn_tn_lines)\n",
    "    \n",
    "    # Update global best if improved\n",
    "    if bestValAccuracyForConfig > maxValAcc:\n",
    "        maxValAcc = bestValAccuracyForConfig\n",
    "        maxValAccFolder = index + 1 # +1 so it matches folder numbering\n",
    "    \n",
    "    # Combine everything into a SINGLE text file        \n",
    "    reportText = []\n",
    "    reportText.append(\"Hyperparameters and Results\\n\" + \"=\"*40)\n",
    "    reportText.append(f\"Batch Size: {batch_size}\")\n",
    "    reportText.append(f\"Image Resize: {resize_size}\")\n",
    "    reportText.append(f\"Learning Rate: {lr}\")\n",
    "    reportText.append(f\"Validation Loss: {val_loss_avg:.4f}\")\n",
    "    reportText.append(f\"Validation Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "    \n",
    "    # Add epochLogs\n",
    "    reportText.append(\"Epoch Logs:\")\n",
    "    reportText.extend(epochLogs)\n",
    "    \n",
    "    reportText.append(f\"\\nGlobal Best Accuracy So Far: {maxValAcc:.4f}%\")\n",
    "    reportText.append(f\"Folder with Global Best Accuracy So Far: {maxValAccFolder}\\n\")      \n",
    "\n",
    "    # Confusion matrix (text-version)\n",
    "    reportText.append(\"Confusion Matrix (text version):\")\n",
    "\n",
    "    maxLabelLength = max(len(lbl) for lbl in valClassNamesSorted)\n",
    "    \n",
    "   # 2) Build the header row\n",
    "    header = \" \" * (maxLabelLength + 2)  # Padding for row labels\n",
    "    for label in valClassNamesSorted:\n",
    "        header += f\"{label:>5} \"  # Each label right-aligned\n",
    "    reportText.append(header)\n",
    "\n",
    "    # 3) Build each row with sorted class labels\n",
    "    for row_label, row_values in zip(valClassNamesSorted, valConfMatrix):\n",
    "        row_str = f\"{row_label:<{maxLabelLength}} \"  # Left-align class label\n",
    "        for val in row_values:\n",
    "            row_str += f\"{val:>5} \"  # Right-align each value\n",
    "        reportText.append(row_str)\n",
    "    reportText.append(\"\")  # Blank line for readability\n",
    "\n",
    "    # Add the classification report\n",
    "    reportText.append(\"Classification Report:\")\n",
    "    reportText.append(classification_report(\n",
    "        valLabelsArray,\n",
    "        valPredsArray,\n",
    "        labels=valClassesInUse_sorted,\n",
    "        targetNames=valClassNamesSorted,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Per-class TP/FP/FN/TN\n",
    "    reportText.append(tp_fp_fn_tn_report)\n",
    "\n",
    "    finalReport = \"\\n\".join(reportText)\n",
    "\n",
    "    # Save to a text file\n",
    "    combinedReportPath = os.path.join(newFolderPath, \"combined_report.txt\")\n",
    "    with open(combinedReportPath, \"w\") as f:\n",
    "        f.write(finalReport)\n",
    "\n",
    "    # Save the model\n",
    "    modelPath = os.path.join(newFolderPath, \"model.pth\")\n",
    "    torch.save(model.state_dict(), modelPath)\n",
    "\n",
    "print(\"Grid search complete!\")\n",
    "print(f\"Best accuracy overall: {maxValAcc:.2f}%\")\n",
    "print(f\"Best folder overall: {maxValAccFolder}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
